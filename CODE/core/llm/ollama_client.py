# If you're doing RAG, the process is:

# User asks a question (e.g., "What is AI?")

# You convert the question to an embedding.

# Search in your document database (like ChromaDB) for similar embeddings (semantically similar text).

# Feed those results to your LLM (Ollama) to answer accurately.

# So, embedding = how you connect the user question to the most relevant document chunks.

import ollama
import os

OLLAMA_HOST = os.getenv('OLLAMA_HOST', '127.0.0.1:11434')
os.environ['OLLAMA_HOST'] = OLLAMA_HOST

def get_embedding(text, model='nomic-embed-text'):
    try:
        text = text.strip()
        if not text:
            return None
        print(f"Generating embedding with model: {model}")
        response = ollama.embeddings(model=model, prompt=text)
        embedding = response.get('embedding') or response.get('embeddings')
        if not embedding:
            raise ValueError(f"Model '{model}' returned no embedding data")
        if len(embedding) != 768:
            raise ValueError(f"Embedding dimension mismatch: expected 768, got {len(embedding)}")
        return embedding
    except Exception as e:
        print(f"Error generating embedding: {e}")
        return None

def chat_with_ollama(query, context_chunks, model='mistral'):
    try:
        print(f"Using chat model: {model} on host: {OLLAMA_HOST}")
        instruction_prompt = (
            "You are a helpful assistant. "
            "Use the following context from documents to provide accurate and helpful answers. "
            "If the answer is not in the context, state 'The document does not contain this specific information'. "
            "Do not answer if the context is not relevant to the user query or No relevant document chunks found. "
            "Context:\n"
        )
        if context_chunks:
            for i, chunk in enumerate(context_chunks, 1):
                instruction_prompt += f"Chunk {i} (Similarity: {chunk['similarity']:.3f}):\n{chunk['text'][:500]}...\n\n"
        else:
            instruction_prompt += "No relevant document chunks found.\n\n"
        
        instruction_prompt += f"User Query: {query}"

        stream = ollama.chat(
            model=model,
            messages=[
                {'role': 'system', 'content': instruction_prompt},
                {'role': 'user', 'content': query},
            ],
            stream=True,
        )

        response = ""
        for chunk in stream:
            if 'message' in chunk and 'content' in chunk['message']:
                response += chunk['message']['content']
        return response.strip() if response else "No response generated by the model."
    except Exception as e:
        print(f"Ollama chat error: {type(e).__name__}: {e}")
        return "Sorry, I couldn't generate a response. Please try again."
